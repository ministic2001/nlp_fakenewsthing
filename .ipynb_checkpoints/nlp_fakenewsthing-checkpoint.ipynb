{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 1: Import the libraries needed for the code.</h2>\n",
    "<p>Import all the needed libraries after you have installed them in the virtual environment.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct version of Tensorflow installed.\n"
     ]
    }
   ],
   "source": [
    "# First we import the required libraries\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#sklearn actually refers to scikit-learn\n",
    "#download that package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "# Check tensorflow version\n",
    "if float(tf.__version__[0]) < 2.0:\n",
    "  print('Updating tensorflow')\n",
    "  !pip install tensorflow==2.0\n",
    "else:\n",
    "  print('Correct version of Tensorflow installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 2: Get the data from the training set.</h2>\n",
    "<p>The training dataset file that we're using is named 'train.csv'.</p>\n",
    "<p>The training and test datasets were obtained from this  <a href=\"https://www.kaggle.com/c/fake-news/data\">link</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        20800\n",
       "title     20800\n",
       "author    20800\n",
       "text      20800\n",
       "label     20800\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the data from the training set\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.fillna(' ')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Now we tokenize the words in the training set.\n",
    "<p>The text will first be tokenized.</p>\n",
    "<p>Following that, it is converted into sequence and then padded to give them a uniform length.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text']) # This encodes all the values in the 'text' column of the dataframe\n",
    "word_index = tokenizer.word_index #returns a dictionary of key value pairs where the key is the word in \n",
    "#the sentence and the value is the label assigned to it. Basically: the words and the values assigned to them\n",
    "vocab_size=len(word_index) \n",
    "#print(vocab_size) #This essentially shows how many values are encoded in total\n",
    "\n",
    "# Padding data\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text']) #converts the sentences to their labelled equivalent based \n",
    "#on the corpus of words passed to it\n",
    "padded = pad_sequences(sequences, maxlen=500, padding='post', truncating='post') #makes the sentences uniform in length\n",
    "#Here, the maximum length of the sentence is defined as 500, and the padding is to be done after the sentence instead of before.\n",
    "#The padding makes each sentence as long as the longest sentence and. It's been set to pad and truncate/shorten at the end of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split the dataset into 2 parts.\n",
    "Now the dataset is to be split into 2 parts: the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "split_n = int(round(len(padded)*(1-split),0))\n",
    "#This gets the number of values in padded and times it\n",
    "\n",
    "#The round function rounds a floating point number to the nearest specified amount of decimals. \n",
    "#The defualt arguement for to what decimal place is 0. At 0, it returns a full integer.\n",
    "\n",
    "#Then it returns split_n as an integer value. This should be 80% of the original number of values\n",
    "\n",
    "train_data = padded[:split_n] #This splits the overall data into the training data for 80% of it\n",
    "train_labels = df['label'].values[:split_n] #This assigns the front 80% of the label to the variable\n",
    "test_data = padded[split_n:] #This splits the overall data into the test data for 20% of it\n",
    "test_labels = df['label'].values[split_n:] #I think this assigns the back 20% of the label to the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 5: Import the glove.6B.100d.txt file into the code.</h2> This file contains values for many words already inside, which we will be feeding to the machine learning code. All the values in the file is then put into an empty array, ignoring any empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensor representations for words\n",
    "\n",
    "embeddings_index = {};\n",
    "with open('glove.6B.100d.txt', encoding=\"utf8\") as f:  #So glove.6B.100d.txt is a file with Pre-trained word vectors, with\n",
    "    #6 billion tokens and 100 features\n",
    "    for line in f:\n",
    "        values = line.split(); #This puts each line into a separate array for each and each word in each line as a separate \n",
    "        #value within each array\n",
    "        word = values[0]; #this assigns the first word in each line to the variable. It is 0 since the word is the first value\n",
    "        #in each line\n",
    "        coefs = np.asarray(values[1:], dtype='float32'); # This assigns all the coefficients for the word in the line to this variable\n",
    "        #The syntax of values should mean that the front part of the second value/first coeff onwards is returned.\n",
    "        embeddings_index[word] = coefs; #This puts all the coefficients into the empty embeddings_index array earlier\n",
    "#print(len(coefs)) #This shows how many coeffs there are for the words. There are 100.\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, 100)); #np.zeros is a function that fills an empty array with zeros, up to the number you\n",
    "#specify for it. The paremeter passed here is the shapes arguement, which defines the shape of the array. Here, it is defined as vocab_size+1\n",
    "#rows by 100 columns. Reminder that here, vocab_size+1 is 238052, so there are 238052 rows by 100 columns. 100 columns, since there are\n",
    "#100 features per row in glove.6B.100d.txt\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word); #The current value of the item in word_index is assigned to this variable.\n",
    "    if embedding_vector is not None: #if the is a value assigned to embedding_vector \n",
    "        embeddings_matrix[i] = embedding_vector; #it gets put into the embeddings_matrix array\n",
    "        #This should effectively weed out any empty values in the dataset and have a more proper dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 6: Build the model.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the architecture of the model\n",
    "     \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, 100, weights=[embeddings_matrix], trainable=False), #.embedding is used as the first layer of \n",
    "    #the model. The size of the vocabolary and number of dimensions are passed, and the weights are assigned here, which cannot be trained i think.\n",
    "    tf.keras.layers.Dropout(0.2), #Dropout will randomly set input units to 0 with a rate to prevent overfitting. The arguement to be \n",
    "    #passed here is the rate: 0.2, which is 20%.\n",
    "    tf.keras.layers.Conv1D(80, 5, activation='relu'), #Cropping layer for 1D\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=5), #the max size of the pooling window is 4 here.\n",
    "    tf.keras.layers.LSTM(20, return_sequences=True), #Long Short-term Memory layer. LSTMs address the vanishing gradient problem that occurs \n",
    "    #when training RNNs. Return_sequences is used to decide whether to return the last output. in the output sequence, or the full sequence.\n",
    "    tf.keras.layers.LSTM(20),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Dense(512), #Densely connected NN layer. The units parameter denotes the output size of the layer\n",
    "    tf.keras.layers.Dropout(0.2),  \n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') #activation arguement is what activation function to use \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 7: Put together the model and see its progress.</h2>\n",
    "\n",
    "<p>I believe this is what the Adam optimizer consists of. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\">Source</a>.</p>\n",
    "<blockquote>\n",
    "    tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "        name='Adam', **kwargs\n",
    "    )\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) #binary_crossentropy computes the cross-entropy loss \n",
    "#between true labels and predicted labels. The optimizer uses the Adam alogorithm here. The class accuracy \n",
    "#calculates accuracy.\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=5, batch_size=100, validation_data=[test_data, test_labels])\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 8: Put together the model of the produced results.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results:\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Include the code for extracting the text from the url here\n",
    "<p>Now here is the code that will obtain the text from the news articles.</p>\n",
    "<p>First, it checks if the inputed data is a valid url with 'check_url()', and the input is rejected if it fails the check.</p>\n",
    "<p>It is then put through 'clean_article()', and returns the text if there were no other errors.</p>\n",
    "<p><em>Note: The variables that are defined as various links at the start of the cell are no longer needed.</em> They are a holdover from when the user input function was not inplemented yet and the code was still in testing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.straitstimes.com/singapore/doxxers-wrongly-identify-tech-ceo-as-sovereign-woman?cx_testId=20&cx_testVariant=cx_1&cx_artPos=2#cxrecs_s\"\n",
    "link2 = \"https://www.slashgear.com/roblox-accounts-got-hacked-by-allegedly-bribing-an-insider-04619242\"\n",
    "link3 = \"https://www.vice.com/en_us/article/qj4ddw/hacker-bribed-roblox-insider-accessed-user-data-reset-passwords\"\n",
    "link4 = \"https://www.straitstimes.com/asia/se-asia/duterte-government-shuts-down-philippines-top-broadcaster-abs-cbn\"\n",
    "link5 = \"https://www.bbc.com/news/amp/science-environment-52547885\"\n",
    "link6 = \"https://www.theonion.com/immune-system-bored-too-1843269164\"\n",
    "link7 = \"https://www.studentnewsdaily.com/daily-news-article/2nd-us-navy-ship-conducts-freedom-of-navigation-operation-china-angry/\"\n",
    "link8 = \"https://www.politifact.com/factchecks/2020/may/05/tom-mcclintock/tom-mcclintock-claimed-flu-killed-80000-americans-/\"\n",
    "link9 = \"https://www.theguardian.com/world/2020/may/09/global-report-trump-says-covid-19-will-go-away-without-vaccine-expects-us-death-toll-to-top-95000\"\n",
    "link10 = \"https://mothership.sg/2020/05/yishun-dish-collector-death-5-days/\"\n",
    "\n",
    "def check_url(website_link):\n",
    "    if len(website_link) > 0:   \n",
    "        if (website_link[:4] == \"http\") & (len(website_link) > 7):\n",
    "            return clean_article(website_link)\n",
    "        else:\n",
    "            return \"Not a website\"    \n",
    "    else:\n",
    "        return \"input is empty\"\n",
    "        \n",
    "def clean_article(website_link):\n",
    "    website_html = requests.get(website_link).text.encode(\"utf-8\").decode(\"windows-1252\", \"ignore\")\n",
    "    #print(website_html)\n",
    "    if len(website_html) > 0:\n",
    "        try:\n",
    "            if len(re.findall(\"<p>.*?</p>\", website_html)) > 0:\n",
    "                article = re.findall(\"<p>.*?</p>\", website_html)\n",
    "            elif len(re.findall(r\"(<p\\s.*?>.*?<\\/p>)\", website_html, re.S)) > 0:\n",
    "                article = re.findall(r\"(<p\\s.*?>.*?<\\/p>)\", website_html, re.S)\n",
    "            elif len(re.findall(\"<p>.*\", website_html)) > 0:\n",
    "                article = re.findall(\"<p>.*\", website_html)\n",
    "            else:\n",
    "                return(website_html)\n",
    "\n",
    "            article = '\\n'.join([str(i) for i in article])\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            article = re.sub(cleanr, '', article) \n",
    "            return article\n",
    "        except UnicodeEncodeError:\n",
    "            return \"Article is not suitable for python to read\"\n",
    "    else:\n",
    "        return \"Article unable to load\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Format the inputed data so that it can be used to make a prediction\n",
    "<p>Here, the user will have to input in a url, and the the code checks if it can be used.</p>\n",
    "<p>Now the text that was obtained is put into a data frame, tokenized and padded, similar to how the training data was formatted.</p>\n",
    "<p>The Model will now predict the likelihood of the article being unreliable using the trained model. The output is rounded to 5 decimal places.</p>\n",
    "<p>And thus, the code can predict how likely a news article is unreliable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = 0\n",
    "while status == 0:\n",
    "    print(\"Please input a link to a news article.\")\n",
    "    hold_input = input()\n",
    "    user_url_input = check_url(hold_input)\n",
    "    #user_url_input = check_url(link6)\n",
    "    #print(user_url_input)\n",
    "    try:\n",
    "        if hold_input == \"cancel\":\n",
    "            print(\"The program will stop now\")\n",
    "            status = 1\n",
    "        elif user_url_input == \"Not a website\" or user_url_input == \"input is empty\" or user_url_input == \"Article is not suitable for python to read\" or user_url_input == \"Article unable to load\":\n",
    "            print(\"This input is not suitable.\")\n",
    "            print(\"Reason: \" + user_url_input)\n",
    "        else:\n",
    "            #So we put the text that was pulled from the site and put it into an array\n",
    "            input_array = np.array([[0, 'random title', 'random writer', user_url_input]])\n",
    "            column_name = ['id', 'title', 'author', 'text']\n",
    "            input_dataframe = pd.DataFrame(data=input_array, columns=column_name) #Then it's formatted into a dataframe\n",
    "\n",
    "            #Similar to when the training data was being formatted, we will tokenize the text in the 'text' field, followed by padding.\n",
    "            tokenizer.fit_on_texts(input_dataframe['text'])\n",
    "            input_sequences = tokenizer.texts_to_sequences(input_dataframe['text'])\n",
    "            input_padded = pad_sequences(input_sequences, maxlen=500, padding='post', truncating='post')\n",
    "\n",
    "            #Now the input data is now formatted similarly to the training data, and is now passed to the model.predict() function.\n",
    "            #ALso, yes. The other values in the dataframe are just for show, but this is the only method I've figured out that didn't cause\n",
    "            #errors upon errors. Feel free to try something else and see if it works.\n",
    "            prediction = model.predict(input_padded) \n",
    "            rounded_prediction = round(prediction[0,0]*100, 5)\n",
    "            output_prediction = \"This is the model's predicted value: {}%\".format(rounded_prediction)\n",
    "            print(output_prediction)\n",
    "\n",
    "            reliability = 100-rounded_prediction\n",
    "            output_reliability = \"This is the model's prediction of how accurate the article provided is: {}%\".format(reliability)\n",
    "            print(output_reliability)    \n",
    "\n",
    "            if rounded_prediction <= 30:\n",
    "                print(\"This article is reliable\")\n",
    "            elif rounded_prediction > 30 and rounded_prediction <= 50:\n",
    "                print(\"This article is likely reliable\")\n",
    "            elif rounded_prediction > 50 and rounded_prediction <= 70:\n",
    "                print(\"This article is likely unreliable\")\n",
    "            else:\n",
    "                print(\"This article is garbage, trash, and not even worthy of a 3rd adjective. Where did you even find this?\")\n",
    "    except:\n",
    "        print(\"An Error has occured. Please try again\")\n",
    "        \n",
    "print(\"It has been stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenvironment",
   "language": "python",
   "name": "tfenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
